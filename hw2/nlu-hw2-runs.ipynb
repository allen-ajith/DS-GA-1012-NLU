{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\n\n# Load the GitHub token from Kaggle Secrets\nuser_secrets = UserSecretsClient()\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")  # Ensure the key matches what you set in Secrets\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-17T01:51:23.751819Z","iopub.execute_input":"2025-03-17T01:51:23.752166Z","iopub.status.idle":"2025-03-17T01:51:23.916405Z","shell.execute_reply.started":"2025-03-17T01:51:23.752135Z","shell.execute_reply":"2025-03-17T01:51:23.915818Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport shutil\n\n# Repository URL\nREPO_URL = \"https://github.com/allen-ajith/DS-GA-1012-NLU.git\"\n\n# Set up the repository URL with the token for authentication\nAUTH_REPO_URL = f\"https://{GITHUB_TOKEN}@{REPO_URL.split('https://')[1]}\"\n\n# Path to the repository folder\nREPO_FOLDER = \"/kaggle/working/DS-GA-1012-NLU\"\n\n# Check if the folder exists and remove it\nif os.path.exists(REPO_FOLDER):\n    print(f\"Removing existing folder: {REPO_FOLDER}\")\n    shutil.rmtree(REPO_FOLDER)\n\n# Clone the repository into the Kaggle working directory\n!git clone {AUTH_REPO_URL}\n\nimport sys\n\n# Add the hw2 directory to the Python path\nsys.path.append(\"/kaggle/working/DS-GA-1012-NLU/hw2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T01:51:23.917373Z","iopub.execute_input":"2025-03-17T01:51:23.917611Z","iopub.status.idle":"2025-03-17T01:51:28.419337Z","shell.execute_reply.started":"2025-03-17T01:51:23.917592Z","shell.execute_reply":"2025-03-17T01:51:28.418285Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'DS-GA-1012-NLU'...\nremote: Enumerating objects: 238, done.\u001b[K\nremote: Counting objects: 100% (7/7), done.\u001b[K\nremote: Compressing objects: 100% (5/5), done.\u001b[K\nremote: Total 238 (delta 1), reused 4 (delta 1), pack-reused 231 (from 2)\u001b[K\nReceiving objects: 100% (238/238), 115.03 MiB | 43.24 MiB/s, done.\nResolving deltas: 100% (110/110), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pwd\n!pip install evaluate\n!pip install optuna\n!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T01:51:28.420926Z","iopub.execute_input":"2025-03-17T01:51:28.421194Z","iopub.status.idle":"2025-03-17T01:51:39.541561Z","shell.execute_reply.started":"2025-03-17T01:51:28.421172Z","shell.execute_reply":"2025-03-17T01:51:39.540680Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nRequirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.2.1)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.1)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\nRequirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\nRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os \nos.environ[\"WANDB_DISABLED\"] = \"true\"\nfrom tqdm.notebook import tqdm\ntqdm._instances.clear()\n!python /kaggle/working/DS-GA-1012-NLU/hw2/train_model_bitfit.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T01:51:39.543164Z","iopub.execute_input":"2025-03-17T01:51:39.543412Z","iopub.status.idle":"2025-03-17T02:29:15.704831Z","shell.execute_reply.started":"2025-03-17T01:51:39.543392Z","shell.execute_reply":"2025-03-17T02:29:15.703811Z"}},"outputs":[{"name":"stdout","text":"2025-03-17 01:51:48.710547: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-17 01:51:48.895843: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-17 01:51:48.948264: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nREADME.md: 100%|███████████████████████████| 7.81k/7.81k [00:00<00:00, 45.9MB/s]\ntrain-00000-of-00001.parquet: 100%|█████████| 21.0M/21.0M [00:00<00:00, 129MB/s]\ntest-00000-of-00001.parquet: 100%|██████████| 20.5M/20.5M [00:00<00:00, 162MB/s]\nunsupervised-00000-of-00001.parquet: 100%|██| 42.0M/42.0M [00:00<00:00, 184MB/s]\nGenerating train split: 100%|██| 25000/25000 [00:00<00:00, 126456.65 examples/s]\nGenerating test split: 100%|███| 25000/25000 [00:00<00:00, 202226.74 examples/s]\nGenerating unsupervised split: 100%|█| 50000/50000 [00:00<00:00, 194066.64 examp\nvocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 9.52MB/s]\nconfig.json: 100%|█████████████████████████████| 285/285 [00:00<00:00, 2.51MB/s]\nMap: 100%|███████████████████████| 20000/20000 [00:13<00:00, 1511.26 examples/s]\nMap: 100%|█████████████████████████| 5000/5000 [00:03<00:00, 1578.93 examples/s]\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\npytorch_model.bin: 100%|████████████████████| 17.8M/17.8M [00:00<00:00, 175MB/s]\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[32m[I 2025-03-17 01:52:25,819]\u001b[0m A new study created in memory with name: no-name-7036d808-90f1-4a82-8404-419b27ad451a\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 16.7MB/s]\n{'eval_loss': 0.690009355545044, 'eval_accuracy': 0.5476, 'eval_runtime': 8.0164, 'eval_samples_per_second': 623.723, 'eval_steps_per_second': 39.045, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6888733506202698, 'eval_accuracy': 0.5546, 'eval_runtime': 7.7153, 'eval_samples_per_second': 648.062, 'eval_steps_per_second': 40.569, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6882460713386536, 'eval_accuracy': 0.5594, 'eval_runtime': 7.9057, 'eval_samples_per_second': 632.458, 'eval_steps_per_second': 39.592, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.689, 'grad_norm': 0.09914976358413696, 'learning_rate': 6.114649681528663e-06, 'epoch': 3.1847133757961785}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6880699992179871, 'eval_accuracy': 0.5612, 'eval_runtime': 7.7663, 'eval_samples_per_second': 643.811, 'eval_steps_per_second': 40.303, 'epoch': 4.0}\n{'train_runtime': 144.9091, 'train_samples_per_second': 552.07, 'train_steps_per_second': 4.334, 'train_loss': 0.688750831944168, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 01:54:51,367]\u001b[0m Trial 0 finished with value: 0.5612 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 64}. Best is trial 0 with value: 0.5612.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nError during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nmodel.safetensors: 100%|███████████████████| 17.7M/17.7M [00:00<00:00, 78.6MB/s]\n{'loss': 0.6812, 'grad_norm': 0.09544093906879425, 'learning_rate': 0.00023999999999999998, 'epoch': 0.8}\n{'eval_loss': 0.67085862159729, 'eval_accuracy': 0.6248, 'eval_runtime': 7.6792, 'eval_samples_per_second': 651.107, 'eval_steps_per_second': 40.759, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6705, 'grad_norm': 0.16414013504981995, 'learning_rate': 0.00017999999999999998, 'epoch': 1.6}\n{'eval_loss': 0.6634149551391602, 'eval_accuracy': 0.6338, 'eval_runtime': 7.7937, 'eval_samples_per_second': 641.542, 'eval_steps_per_second': 40.161, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6658, 'grad_norm': 0.09460526704788208, 'learning_rate': 0.00011999999999999999, 'epoch': 2.4}\n{'eval_loss': 0.6611906290054321, 'eval_accuracy': 0.6282, 'eval_runtime': 7.678, 'eval_samples_per_second': 651.215, 'eval_steps_per_second': 40.766, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6637, 'grad_norm': 0.0829949676990509, 'learning_rate': 5.9999999999999995e-05, 'epoch': 3.2}\n{'loss': 0.6605, 'grad_norm': 0.09442219883203506, 'learning_rate': 0.0, 'epoch': 4.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6600612998008728, 'eval_accuracy': 0.6302, 'eval_runtime': 7.8598, 'eval_samples_per_second': 636.151, 'eval_steps_per_second': 39.823, 'epoch': 4.0}\n{'train_runtime': 163.482, 'train_samples_per_second': 489.351, 'train_steps_per_second': 15.292, 'train_loss': 0.668319384765625, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 01:57:35,601]\u001b[0m Trial 1 finished with value: 0.6302 and parameters: {'learning_rate': 0.0003, 'per_device_train_batch_size': 16}. Best is trial 1 with value: 0.6302.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6907480359077454, 'eval_accuracy': 0.5412, 'eval_runtime': 7.7246, 'eval_samples_per_second': 647.283, 'eval_steps_per_second': 40.52, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6899945139884949, 'eval_accuracy': 0.545, 'eval_runtime': 7.7058, 'eval_samples_per_second': 648.861, 'eval_steps_per_second': 40.619, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6895380616188049, 'eval_accuracy': 0.5508, 'eval_runtime': 7.6869, 'eval_samples_per_second': 650.456, 'eval_steps_per_second': 40.719, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6894096732139587, 'eval_accuracy': 0.5496, 'eval_runtime': 7.6979, 'eval_samples_per_second': 649.53, 'eval_steps_per_second': 40.661, 'epoch': 4.0}\n{'train_runtime': 144.2463, 'train_samples_per_second': 554.607, 'train_steps_per_second': 2.191, 'train_loss': 0.6901865126211432, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:00:00,509]\u001b[0m Trial 2 finished with value: 0.5496 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 128}. Best is trial 1 with value: 0.6302.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6867579817771912, 'eval_accuracy': 0.5638, 'eval_runtime': 7.6976, 'eval_samples_per_second': 649.555, 'eval_steps_per_second': 40.662, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6840307712554932, 'eval_accuracy': 0.5864, 'eval_runtime': 8.0011, 'eval_samples_per_second': 624.912, 'eval_steps_per_second': 39.119, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6822876334190369, 'eval_accuracy': 0.596, 'eval_runtime': 7.7519, 'eval_samples_per_second': 645.007, 'eval_steps_per_second': 40.377, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6854, 'grad_norm': 0.10830490291118622, 'learning_rate': 2.0382165605095544e-05, 'epoch': 3.1847133757961785}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6818206310272217, 'eval_accuracy': 0.5982, 'eval_runtime': 7.6986, 'eval_samples_per_second': 649.467, 'eval_steps_per_second': 40.657, 'epoch': 4.0}\n{'train_runtime': 144.6142, 'train_samples_per_second': 553.196, 'train_steps_per_second': 4.343, 'train_loss': 0.6847214425445363, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:02:25,815]\u001b[0m Trial 3 finished with value: 0.5982 and parameters: {'learning_rate': 0.0001, 'per_device_train_batch_size': 64}. Best is trial 1 with value: 0.6302.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6894, 'grad_norm': 0.25532063841819763, 'learning_rate': 4.5e-05, 'epoch': 0.4}\n{'loss': 0.6866, 'grad_norm': 0.07732819765806198, 'learning_rate': 4e-05, 'epoch': 0.8}\n{'eval_loss': 0.6837530136108398, 'eval_accuracy': 0.585, 'eval_runtime': 7.9466, 'eval_samples_per_second': 629.201, 'eval_steps_per_second': 39.388, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.684, 'grad_norm': 0.11268294602632523, 'learning_rate': 3.5e-05, 'epoch': 1.2}\n{'loss': 0.6817, 'grad_norm': 0.0952748954296112, 'learning_rate': 3e-05, 'epoch': 1.6}\n{'loss': 0.6803, 'grad_norm': 0.1530454307794571, 'learning_rate': 2.5e-05, 'epoch': 2.0}\n{'eval_loss': 0.6789977550506592, 'eval_accuracy': 0.609, 'eval_runtime': 7.6744, 'eval_samples_per_second': 651.518, 'eval_steps_per_second': 40.785, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.679, 'grad_norm': 0.14556747674942017, 'learning_rate': 2e-05, 'epoch': 2.4}\n{'loss': 0.679, 'grad_norm': 0.20063811540603638, 'learning_rate': 1.5e-05, 'epoch': 2.8}\n{'eval_loss': 0.676748514175415, 'eval_accuracy': 0.616, 'eval_runtime': 7.7979, 'eval_samples_per_second': 641.201, 'eval_steps_per_second': 40.139, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6782, 'grad_norm': 0.07959290593862534, 'learning_rate': 1e-05, 'epoch': 3.2}\n{'loss': 0.677, 'grad_norm': 0.1226399764418602, 'learning_rate': 5e-06, 'epoch': 3.6}\n{'loss': 0.6763, 'grad_norm': 0.14714714884757996, 'learning_rate': 0.0, 'epoch': 4.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6761190295219421, 'eval_accuracy': 0.6184, 'eval_runtime': 7.857, 'eval_samples_per_second': 636.377, 'eval_steps_per_second': 39.837, 'epoch': 4.0}\n{'train_runtime': 204.2895, 'train_samples_per_second': 391.601, 'train_steps_per_second': 24.475, 'train_loss': 0.6811561645507812, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:05:51,009]\u001b[0m Trial 4 finished with value: 0.6184 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 8}. Best is trial 1 with value: 0.6302.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nError during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6890983581542969, 'eval_accuracy': 0.554, 'eval_runtime': 7.9159, 'eval_samples_per_second': 631.637, 'eval_steps_per_second': 39.54, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6894, 'grad_norm': 0.06351051479578018, 'learning_rate': 1.8019169329073483e-05, 'epoch': 1.5974440894568689}\n{'eval_loss': 0.6875524520874023, 'eval_accuracy': 0.5594, 'eval_runtime': 7.7172, 'eval_samples_per_second': 647.9, 'eval_steps_per_second': 40.559, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6866816282272339, 'eval_accuracy': 0.5648, 'eval_runtime': 7.6684, 'eval_samples_per_second': 652.026, 'eval_steps_per_second': 40.817, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6869, 'grad_norm': 0.04790499806404114, 'learning_rate': 6.038338658146965e-06, 'epoch': 3.194888178913738}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6864305734634399, 'eval_accuracy': 0.5664, 'eval_runtime': 7.6686, 'eval_samples_per_second': 652.011, 'eval_steps_per_second': 40.816, 'epoch': 4.0}\n{'train_runtime': 148.5074, 'train_samples_per_second': 538.694, 'train_steps_per_second': 8.431, 'train_loss': 0.6878250536446373, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:08:20,167]\u001b[0m Trial 5 finished with value: 0.5664 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32}. Best is trial 1 with value: 0.6302.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6867, 'grad_norm': 0.05674600973725319, 'learning_rate': 8e-05, 'epoch': 0.8}\n{'eval_loss': 0.6818881630897522, 'eval_accuracy': 0.5966, 'eval_runtime': 7.6865, 'eval_samples_per_second': 650.49, 'eval_steps_per_second': 40.721, 'epoch': 1.0}\n\u001b[32m[I 2025-03-17 02:09:01,534]\u001b[0m Trial 6 pruned. \u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nError during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6889215707778931, 'eval_accuracy': 0.5558, 'eval_runtime': 7.7987, 'eval_samples_per_second': 641.132, 'eval_steps_per_second': 40.135, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6873294711112976, 'eval_accuracy': 0.5614, 'eval_runtime': 7.8648, 'eval_samples_per_second': 635.746, 'eval_steps_per_second': 39.798, 'epoch': 2.0}\n\u001b[32m[I 2025-03-17 02:10:14,829]\u001b[0m Trial 7 pruned. \u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6835289001464844, 'eval_accuracy': 0.5902, 'eval_runtime': 7.7812, 'eval_samples_per_second': 642.573, 'eval_steps_per_second': 40.225, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6786054968833923, 'eval_accuracy': 0.61, 'eval_runtime': 7.6944, 'eval_samples_per_second': 649.822, 'eval_steps_per_second': 40.679, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6759330630302429, 'eval_accuracy': 0.6186, 'eval_runtime': 7.7158, 'eval_samples_per_second': 648.018, 'eval_steps_per_second': 40.566, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.675285279750824, 'eval_accuracy': 0.6186, 'eval_runtime': 7.6467, 'eval_samples_per_second': 653.876, 'eval_steps_per_second': 40.933, 'epoch': 4.0}\n{'train_runtime': 144.6466, 'train_samples_per_second': 553.072, 'train_steps_per_second': 2.185, 'train_loss': 0.6810917914668216, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:12:40,196]\u001b[0m Trial 8 finished with value: 0.6186 and parameters: {'learning_rate': 0.0003, 'per_device_train_batch_size': 128}. Best is trial 1 with value: 0.6302.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6742232441902161, 'eval_accuracy': 0.6232, 'eval_runtime': 7.7634, 'eval_samples_per_second': 644.049, 'eval_steps_per_second': 40.317, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.679, 'grad_norm': 0.06083342432975769, 'learning_rate': 0.0001801916932907348, 'epoch': 1.5974440894568689}\n{'eval_loss': 0.6683570146560669, 'eval_accuracy': 0.629, 'eval_runtime': 7.6818, 'eval_samples_per_second': 650.891, 'eval_steps_per_second': 40.746, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6660897731781006, 'eval_accuracy': 0.6272, 'eval_runtime': 7.6743, 'eval_samples_per_second': 651.522, 'eval_steps_per_second': 40.785, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6693, 'grad_norm': 0.05827048793435097, 'learning_rate': 6.038338658146964e-05, 'epoch': 3.194888178913738}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6657673716545105, 'eval_accuracy': 0.6272, 'eval_runtime': 8.1827, 'eval_samples_per_second': 611.049, 'eval_steps_per_second': 38.252, 'epoch': 4.0}\n{'train_runtime': 148.2654, 'train_samples_per_second': 539.573, 'train_steps_per_second': 8.444, 'train_loss': 0.6725567704953325, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:15:14,841]\u001b[0m Trial 9 finished with value: 0.6272 and parameters: {'learning_rate': 0.0003, 'per_device_train_batch_size': 32}. Best is trial 1 with value: 0.6302.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6879, 'grad_norm': 0.25735777616500854, 'learning_rate': 9e-05, 'epoch': 0.4}\n{'loss': 0.6827, 'grad_norm': 0.08212971687316895, 'learning_rate': 8e-05, 'epoch': 0.8}\n{'eval_loss': 0.6772890686988831, 'eval_accuracy': 0.613, 'eval_runtime': 7.672, 'eval_samples_per_second': 651.72, 'eval_steps_per_second': 40.798, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6783, 'grad_norm': 0.107601098716259, 'learning_rate': 7e-05, 'epoch': 1.2}\n{'loss': 0.6758, 'grad_norm': 0.09190165996551514, 'learning_rate': 6e-05, 'epoch': 1.6}\n{'loss': 0.6743, 'grad_norm': 0.16476111114025116, 'learning_rate': 5e-05, 'epoch': 2.0}\n{'eval_loss': 0.6718664169311523, 'eval_accuracy': 0.626, 'eval_runtime': 7.7428, 'eval_samples_per_second': 645.765, 'eval_steps_per_second': 40.425, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6724, 'grad_norm': 0.14217789471149445, 'learning_rate': 4e-05, 'epoch': 2.4}\n{'loss': 0.6727, 'grad_norm': 0.24593546986579895, 'learning_rate': 3e-05, 'epoch': 2.8}\n{'eval_loss': 0.6699094772338867, 'eval_accuracy': 0.6246, 'eval_runtime': 7.6546, 'eval_samples_per_second': 653.198, 'eval_steps_per_second': 40.89, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6719, 'grad_norm': 0.08405516296625137, 'learning_rate': 2e-05, 'epoch': 3.2}\n{'loss': 0.6701, 'grad_norm': 0.14622661471366882, 'learning_rate': 1e-05, 'epoch': 3.6}\n{'loss': 0.6692, 'grad_norm': 0.17105911672115326, 'learning_rate': 0.0, 'epoch': 4.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6692476868629456, 'eval_accuracy': 0.6258, 'eval_runtime': 7.8071, 'eval_samples_per_second': 640.446, 'eval_steps_per_second': 40.092, 'epoch': 4.0}\n{'train_runtime': 204.2695, 'train_samples_per_second': 391.639, 'train_steps_per_second': 24.477, 'train_loss': 0.6755369934082032, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:18:39,778]\u001b[0m Trial 10 finished with value: 0.6258 and parameters: {'learning_rate': 0.0001, 'per_device_train_batch_size': 8}. Best is trial 1 with value: 0.6302.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6900110244750977, 'eval_accuracy': 0.546, 'eval_runtime': 7.6973, 'eval_samples_per_second': 649.578, 'eval_steps_per_second': 40.664, 'epoch': 1.0}\n\u001b[32m[I 2025-03-17 02:19:16,596]\u001b[0m Trial 11 pruned. \u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6884106397628784, 'eval_accuracy': 0.5586, 'eval_runtime': 7.89, 'eval_samples_per_second': 633.717, 'eval_steps_per_second': 39.671, 'epoch': 1.0}\n\u001b[32m[I 2025-03-17 02:19:53,383]\u001b[0m Trial 12 pruned. \u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.684718906879425, 'eval_accuracy': 0.578, 'eval_runtime': 7.681, 'eval_samples_per_second': 650.957, 'eval_steps_per_second': 40.75, 'epoch': 1.0}\n\u001b[32m[I 2025-03-17 02:20:31,334]\u001b[0m Trial 13 pruned. \u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6894, 'grad_norm': 0.07058410346508026, 'learning_rate': 2.4e-05, 'epoch': 0.8}\n{'eval_loss': 0.6881452202796936, 'eval_accuracy': 0.558, 'eval_runtime': 7.8243, 'eval_samples_per_second': 639.035, 'eval_steps_per_second': 40.004, 'epoch': 1.0}\n\u001b[32m[I 2025-03-17 02:21:12,908]\u001b[0m Trial 14 pruned. \u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6885, 'grad_norm': 0.06152060627937317, 'learning_rate': 4e-05, 'epoch': 0.8}\n{'eval_loss': 0.6862905025482178, 'eval_accuracy': 0.567, 'eval_runtime': 7.7792, 'eval_samples_per_second': 642.738, 'eval_steps_per_second': 40.235, 'epoch': 1.0}\n\u001b[32m[I 2025-03-17 02:21:54,355]\u001b[0m Trial 15 pruned. \u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nError during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6793595552444458, 'eval_accuracy': 0.6106, 'eval_runtime': 7.824, 'eval_samples_per_second': 639.055, 'eval_steps_per_second': 40.005, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6732961535453796, 'eval_accuracy': 0.6234, 'eval_runtime': 7.7081, 'eval_samples_per_second': 648.67, 'eval_steps_per_second': 40.607, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6709390878677368, 'eval_accuracy': 0.6264, 'eval_runtime': 7.6827, 'eval_samples_per_second': 650.812, 'eval_steps_per_second': 40.741, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.678, 'grad_norm': 0.11313328146934509, 'learning_rate': 6.114649681528662e-05, 'epoch': 3.1847133757961785}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6704477071762085, 'eval_accuracy': 0.6234, 'eval_runtime': 7.6301, 'eval_samples_per_second': 655.304, 'eval_steps_per_second': 41.022, 'epoch': 4.0}\n{'train_runtime': 144.1329, 'train_samples_per_second': 555.043, 'train_steps_per_second': 4.357, 'train_loss': 0.6766823568161885, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:24:19,180]\u001b[0m Trial 16 finished with value: 0.6234 and parameters: {'learning_rate': 0.0003, 'per_device_train_batch_size': 64}. Best is trial 1 with value: 0.6302.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6834, 'grad_norm': 0.28121545910835266, 'learning_rate': 0.00027, 'epoch': 0.4}\n{'loss': 0.6726, 'grad_norm': 0.10646095871925354, 'learning_rate': 0.00023999999999999998, 'epoch': 0.8}\n{'eval_loss': 0.6677749156951904, 'eval_accuracy': 0.6186, 'eval_runtime': 7.7405, 'eval_samples_per_second': 645.953, 'eval_steps_per_second': 40.437, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6677, 'grad_norm': 0.15177254378795624, 'learning_rate': 0.00020999999999999998, 'epoch': 1.2}\n{'loss': 0.6637, 'grad_norm': 0.1059708222746849, 'learning_rate': 0.00017999999999999998, 'epoch': 1.6}\n{'loss': 0.6612, 'grad_norm': 0.2108246386051178, 'learning_rate': 0.00015, 'epoch': 2.0}\n{'eval_loss': 0.6575875282287598, 'eval_accuracy': 0.6336, 'eval_runtime': 7.9205, 'eval_samples_per_second': 631.272, 'eval_steps_per_second': 39.518, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6577, 'grad_norm': 0.15202444791793823, 'learning_rate': 0.00011999999999999999, 'epoch': 2.4}\n{'loss': 0.6589, 'grad_norm': 0.3550361096858978, 'learning_rate': 8.999999999999999e-05, 'epoch': 2.8}\n{'eval_loss': 0.6558330059051514, 'eval_accuracy': 0.6294, 'eval_runtime': 7.6547, 'eval_samples_per_second': 653.194, 'eval_steps_per_second': 40.89, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6575, 'grad_norm': 0.11902687698602676, 'learning_rate': 5.9999999999999995e-05, 'epoch': 3.2}\n{'loss': 0.6542, 'grad_norm': 0.26134106516838074, 'learning_rate': 2.9999999999999997e-05, 'epoch': 3.6}\n{'loss': 0.6535, 'grad_norm': 0.22099092602729797, 'learning_rate': 0.0, 'epoch': 4.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.6540665030479431, 'eval_accuracy': 0.6344, 'eval_runtime': 7.6684, 'eval_samples_per_second': 652.025, 'eval_steps_per_second': 40.817, 'epoch': 4.0}\n{'train_runtime': 203.904, 'train_samples_per_second': 392.341, 'train_steps_per_second': 24.521, 'train_loss': 0.6630355590820313, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:27:44,089]\u001b[0m Trial 17 finished with value: 0.6344 and parameters: {'learning_rate': 0.0003, 'per_device_train_batch_size': 8}. Best is trial 17 with value: 0.6344.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nError during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n{'loss': 0.6902, 'grad_norm': 0.25395217537879944, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.4}\n{'loss': 0.6883, 'grad_norm': 0.0805768370628357, 'learning_rate': 2.4e-05, 'epoch': 0.8}\n{'eval_loss': 0.6864701509475708, 'eval_accuracy': 0.5672, 'eval_runtime': 7.8318, 'eval_samples_per_second': 638.426, 'eval_steps_per_second': 39.965, 'epoch': 1.0}\n\u001b[32m[I 2025-03-17 02:28:35,570]\u001b[0m Trial 18 pruned. \u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.687660813331604, 'eval_accuracy': 0.5598, 'eval_runtime': 7.8186, 'eval_samples_per_second': 639.498, 'eval_steps_per_second': 40.033, 'epoch': 1.0}\n\u001b[32m[I 2025-03-17 02:29:13,406]\u001b[0m Trial 19 pruned. \u001b[0m\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nfrom tqdm.notebook import tqdm\ntqdm._instances.clear()\n!python /kaggle/working/DS-GA-1012-NLU/hw2/train_model_nobitfit.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T02:29:15.706111Z","iopub.execute_input":"2025-03-17T02:29:15.706385Z","iopub.status.idle":"2025-03-17T03:08:37.896824Z","shell.execute_reply.started":"2025-03-17T02:29:15.706358Z","shell.execute_reply":"2025-03-17T03:08:37.895841Z"}},"outputs":[{"name":"stdout","text":"2025-03-17 02:29:19.795890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-17 02:29:19.822084: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-17 02:29:19.828871: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nMap: 100%|███████████████████████| 20000/20000 [00:13<00:00, 1511.84 examples/s]\nMap: 100%|█████████████████████████| 5000/5000 [00:03<00:00, 1581.51 examples/s]\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[32m[I 2025-03-17 02:29:43,871]\u001b[0m A new study created in memory with name: no-name-5dcde751-ea0f-4839-8169-3dcd3ecc8b4c\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.5736774206161499, 'eval_accuracy': 0.714, 'eval_runtime': 7.5159, 'eval_samples_per_second': 665.255, 'eval_steps_per_second': 41.645, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.46182742714881897, 'eval_accuracy': 0.8004, 'eval_runtime': 7.5402, 'eval_samples_per_second': 663.109, 'eval_steps_per_second': 41.511, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.4278963804244995, 'eval_accuracy': 0.814, 'eval_runtime': 7.6514, 'eval_samples_per_second': 653.478, 'eval_steps_per_second': 40.908, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.5203, 'grad_norm': 3.030806303024292, 'learning_rate': 6.114649681528663e-06, 'epoch': 3.1847133757961785}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.4172353148460388, 'eval_accuracy': 0.8188, 'eval_runtime': 7.7293, 'eval_samples_per_second': 646.888, 'eval_steps_per_second': 40.495, 'epoch': 4.0}\n{'train_runtime': 147.6947, 'train_samples_per_second': 541.658, 'train_steps_per_second': 4.252, 'train_loss': 0.4952882414410828, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:32:12,198]\u001b[0m Trial 0 finished with value: 0.8188 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 64}. Best is trial 0 with value: 0.8188.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.3956, 'grad_norm': 10.078343391418457, 'learning_rate': 0.00023999999999999998, 'epoch': 0.8}\n{'eval_loss': 0.28946933150291443, 'eval_accuracy': 0.8772, 'eval_runtime': 7.6467, 'eval_samples_per_second': 653.88, 'eval_steps_per_second': 40.933, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.2453, 'grad_norm': 3.2044517993927, 'learning_rate': 0.00017999999999999998, 'epoch': 1.6}\n{'eval_loss': 0.33986690640449524, 'eval_accuracy': 0.8708, 'eval_runtime': 7.5324, 'eval_samples_per_second': 663.802, 'eval_steps_per_second': 41.554, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.1625, 'grad_norm': 6.142711639404297, 'learning_rate': 0.00011999999999999999, 'epoch': 2.4}\n{'eval_loss': 0.3660997450351715, 'eval_accuracy': 0.8868, 'eval_runtime': 7.7279, 'eval_samples_per_second': 647.006, 'eval_steps_per_second': 40.503, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.1137, 'grad_norm': 6.007303714752197, 'learning_rate': 5.9999999999999995e-05, 'epoch': 3.2}\n{'loss': 0.0665, 'grad_norm': 0.11533740162849426, 'learning_rate': 0.0, 'epoch': 4.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.46571630239486694, 'eval_accuracy': 0.8878, 'eval_runtime': 7.7264, 'eval_samples_per_second': 647.129, 'eval_steps_per_second': 40.51, 'epoch': 4.0}\n{'train_runtime': 166.143, 'train_samples_per_second': 481.513, 'train_steps_per_second': 15.047, 'train_loss': 0.1967194854736328, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:34:59,027]\u001b[0m Trial 1 finished with value: 0.8878 and parameters: {'learning_rate': 0.0003, 'per_device_train_batch_size': 16}. Best is trial 1 with value: 0.8878.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nError during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n{'eval_loss': 0.6230493187904358, 'eval_accuracy': 0.685, 'eval_runtime': 7.4902, 'eval_samples_per_second': 667.536, 'eval_steps_per_second': 41.788, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.5485010743141174, 'eval_accuracy': 0.7446, 'eval_runtime': 7.6712, 'eval_samples_per_second': 651.791, 'eval_steps_per_second': 40.802, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.501893937587738, 'eval_accuracy': 0.7774, 'eval_runtime': 7.5151, 'eval_samples_per_second': 665.328, 'eval_steps_per_second': 41.65, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.4948329031467438, 'eval_accuracy': 0.7778, 'eval_runtime': 7.6418, 'eval_samples_per_second': 654.297, 'eval_steps_per_second': 40.959, 'epoch': 4.0}\n{'train_runtime': 146.5385, 'train_samples_per_second': 545.932, 'train_steps_per_second': 2.156, 'train_loss': 0.5660513382923754, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:37:26,454]\u001b[0m Trial 2 finished with value: 0.7778 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 128}. Best is trial 1 with value: 0.8878.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.4010598659515381, 'eval_accuracy': 0.8222, 'eval_runtime': 7.751, 'eval_samples_per_second': 645.08, 'eval_steps_per_second': 40.382, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.3209507167339325, 'eval_accuracy': 0.862, 'eval_runtime': 7.8154, 'eval_samples_per_second': 639.759, 'eval_steps_per_second': 40.049, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.34280335903167725, 'eval_accuracy': 0.8576, 'eval_runtime': 7.5921, 'eval_samples_per_second': 658.583, 'eval_steps_per_second': 41.227, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.3812, 'grad_norm': 5.702438831329346, 'learning_rate': 2.0382165605095544e-05, 'epoch': 3.1847133757961785}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.3159177601337433, 'eval_accuracy': 0.8746, 'eval_runtime': 7.6061, 'eval_samples_per_second': 657.37, 'eval_steps_per_second': 41.151, 'epoch': 4.0}\n{'train_runtime': 147.0136, 'train_samples_per_second': 544.167, 'train_steps_per_second': 4.272, 'train_loss': 0.355672957790885, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:39:54,173]\u001b[0m Trial 3 finished with value: 0.8746 and parameters: {'learning_rate': 0.0001, 'per_device_train_batch_size': 64}. Best is trial 1 with value: 0.8878.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.5586, 'grad_norm': 11.408890724182129, 'learning_rate': 4.5e-05, 'epoch': 0.4}\n{'loss': 0.391, 'grad_norm': 13.56567668914795, 'learning_rate': 4e-05, 'epoch': 0.8}\n{'eval_loss': 0.3537631928920746, 'eval_accuracy': 0.8474, 'eval_runtime': 7.7932, 'eval_samples_per_second': 641.587, 'eval_steps_per_second': 40.163, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.3551, 'grad_norm': 19.667299270629883, 'learning_rate': 3.5e-05, 'epoch': 1.2}\n{'loss': 0.3086, 'grad_norm': 7.010272979736328, 'learning_rate': 3e-05, 'epoch': 1.6}\n{'loss': 0.3013, 'grad_norm': 53.814762115478516, 'learning_rate': 2.5e-05, 'epoch': 2.0}\n{'eval_loss': 0.348827600479126, 'eval_accuracy': 0.8634, 'eval_runtime': 7.4769, 'eval_samples_per_second': 668.722, 'eval_steps_per_second': 41.862, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.2635, 'grad_norm': 8.394267082214355, 'learning_rate': 2e-05, 'epoch': 2.4}\n{'loss': 0.2534, 'grad_norm': 2.8365254402160645, 'learning_rate': 1.5e-05, 'epoch': 2.8}\n{'eval_loss': 0.3272361755371094, 'eval_accuracy': 0.876, 'eval_runtime': 7.5247, 'eval_samples_per_second': 664.475, 'eval_steps_per_second': 41.596, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.2534, 'grad_norm': 5.562078475952148, 'learning_rate': 1e-05, 'epoch': 3.2}\n{'loss': 0.2283, 'grad_norm': 9.196033477783203, 'learning_rate': 5e-06, 'epoch': 3.6}\n{'loss': 0.2206, 'grad_norm': 37.108314514160156, 'learning_rate': 0.0, 'epoch': 4.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.33200088143348694, 'eval_accuracy': 0.8802, 'eval_runtime': 7.8102, 'eval_samples_per_second': 640.192, 'eval_steps_per_second': 40.076, 'epoch': 4.0}\n{'train_runtime': 209.119, 'train_samples_per_second': 382.557, 'train_steps_per_second': 23.91, 'train_loss': 0.31337914581298826, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:43:24,003]\u001b[0m Trial 4 finished with value: 0.8802 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 8}. Best is trial 1 with value: 0.8878.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nError during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n{'eval_loss': 0.4952771067619324, 'eval_accuracy': 0.78, 'eval_runtime': 7.5272, 'eval_samples_per_second': 664.26, 'eval_steps_per_second': 41.583, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.5438, 'grad_norm': 4.613186836242676, 'learning_rate': 1.8019169329073483e-05, 'epoch': 1.5974440894568689}\n{'eval_loss': 0.3887985646724701, 'eval_accuracy': 0.832, 'eval_runtime': 7.5542, 'eval_samples_per_second': 661.886, 'eval_steps_per_second': 41.434, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.3909260034561157, 'eval_accuracy': 0.832, 'eval_runtime': 7.5764, 'eval_samples_per_second': 659.943, 'eval_steps_per_second': 41.312, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.3757, 'grad_norm': 3.106363534927368, 'learning_rate': 6.038338658146965e-06, 'epoch': 3.194888178913738}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.3712148070335388, 'eval_accuracy': 0.8434, 'eval_runtime': 7.6722, 'eval_samples_per_second': 651.7, 'eval_steps_per_second': 40.796, 'epoch': 4.0}\n{'train_runtime': 150.5393, 'train_samples_per_second': 531.423, 'train_steps_per_second': 8.317, 'train_loss': 0.4362299678424677, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:45:55,384]\u001b[0m Trial 5 finished with value: 0.8434 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 32}. Best is trial 1 with value: 0.8878.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.4477, 'grad_norm': 11.631718635559082, 'learning_rate': 8e-05, 'epoch': 0.8}\n{'eval_loss': 0.31690478324890137, 'eval_accuracy': 0.8672, 'eval_runtime': 7.8141, 'eval_samples_per_second': 639.869, 'eval_steps_per_second': 40.056, 'epoch': 1.0}\n\u001b[32m[I 2025-03-17 02:46:37,669]\u001b[0m Trial 6 pruned. \u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.5022243857383728, 'eval_accuracy': 0.7674, 'eval_runtime': 7.6917, 'eval_samples_per_second': 650.048, 'eval_steps_per_second': 40.693, 'epoch': 1.0}\n\u001b[32m[I 2025-03-17 02:47:14,930]\u001b[0m Trial 7 pruned. \u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.3285597562789917, 'eval_accuracy': 0.861, 'eval_runtime': 7.6037, 'eval_samples_per_second': 657.579, 'eval_steps_per_second': 41.164, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.29306694865226746, 'eval_accuracy': 0.8798, 'eval_runtime': 7.7326, 'eval_samples_per_second': 646.617, 'eval_steps_per_second': 40.478, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.32273730635643005, 'eval_accuracy': 0.8728, 'eval_runtime': 7.5837, 'eval_samples_per_second': 659.307, 'eval_steps_per_second': 41.273, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.312362402677536, 'eval_accuracy': 0.884, 'eval_runtime': 7.7569, 'eval_samples_per_second': 644.585, 'eval_steps_per_second': 40.351, 'epoch': 4.0}\n{'train_runtime': 147.8387, 'train_samples_per_second': 541.13, 'train_steps_per_second': 2.137, 'train_loss': 0.28735274302808544, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:49:43,471]\u001b[0m Trial 8 finished with value: 0.884 and parameters: {'learning_rate': 0.0003, 'per_device_train_batch_size': 128}. Best is trial 1 with value: 0.8878.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.29273146390914917, 'eval_accuracy': 0.881, 'eval_runtime': 8.5836, 'eval_samples_per_second': 582.504, 'eval_steps_per_second': 36.465, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.3499, 'grad_norm': 3.4884495735168457, 'learning_rate': 0.0001801916932907348, 'epoch': 1.5974440894568689}\n{'eval_loss': 0.2907331883907318, 'eval_accuracy': 0.889, 'eval_runtime': 7.6758, 'eval_samples_per_second': 651.4, 'eval_steps_per_second': 40.778, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.31373897194862366, 'eval_accuracy': 0.8872, 'eval_runtime': 7.5636, 'eval_samples_per_second': 661.06, 'eval_steps_per_second': 41.382, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.1573, 'grad_norm': 1.591548204421997, 'learning_rate': 6.038338658146964e-05, 'epoch': 3.194888178913738}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.37672021985054016, 'eval_accuracy': 0.8892, 'eval_runtime': 7.8414, 'eval_samples_per_second': 637.642, 'eval_steps_per_second': 39.916, 'epoch': 4.0}\n{'train_runtime': 153.4378, 'train_samples_per_second': 521.384, 'train_steps_per_second': 8.16, 'train_loss': 0.2202613513690595, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:52:17,618]\u001b[0m Trial 9 finished with value: 0.8892 and parameters: {'learning_rate': 0.0003, 'per_device_train_batch_size': 32}. Best is trial 9 with value: 0.8892.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.4923, 'grad_norm': 10.932331085205078, 'learning_rate': 9e-05, 'epoch': 0.4}\n{'loss': 0.3558, 'grad_norm': 8.481306076049805, 'learning_rate': 8e-05, 'epoch': 0.8}\n{'eval_loss': 0.3028315305709839, 'eval_accuracy': 0.8734, 'eval_runtime': 7.626, 'eval_samples_per_second': 655.651, 'eval_steps_per_second': 41.044, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.3184, 'grad_norm': 12.396384239196777, 'learning_rate': 7e-05, 'epoch': 1.2}\n{'loss': 0.2622, 'grad_norm': 9.38190746307373, 'learning_rate': 6e-05, 'epoch': 1.6}\n{'loss': 0.2557, 'grad_norm': 47.65399932861328, 'learning_rate': 5e-05, 'epoch': 2.0}\n{'eval_loss': 0.36887186765670776, 'eval_accuracy': 0.869, 'eval_runtime': 7.6728, 'eval_samples_per_second': 651.655, 'eval_steps_per_second': 40.794, 'epoch': 2.0}\n\u001b[32m[I 2025-03-17 02:54:03,390]\u001b[0m Trial 10 pruned. \u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nError during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n{'eval_loss': 0.5811490416526794, 'eval_accuracy': 0.7166, 'eval_runtime': 7.5833, 'eval_samples_per_second': 659.346, 'eval_steps_per_second': 41.275, 'epoch': 1.0}\n\u001b[32m[I 2025-03-17 02:54:40,528]\u001b[0m Trial 11 pruned. \u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.4718271493911743, 'eval_accuracy': 0.7882, 'eval_runtime': 7.5672, 'eval_samples_per_second': 660.751, 'eval_steps_per_second': 41.363, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.3626496195793152, 'eval_accuracy': 0.8482, 'eval_runtime': 7.591, 'eval_samples_per_second': 658.674, 'eval_steps_per_second': 41.233, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.34753668308258057, 'eval_accuracy': 0.8536, 'eval_runtime': 7.696, 'eval_samples_per_second': 649.687, 'eval_steps_per_second': 40.67, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.3422425091266632, 'eval_accuracy': 0.8596, 'eval_runtime': 7.6739, 'eval_samples_per_second': 651.556, 'eval_steps_per_second': 40.787, 'epoch': 4.0}\n{'train_runtime': 147.0953, 'train_samples_per_second': 543.865, 'train_steps_per_second': 2.148, 'train_loss': 0.40104298651972903, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:57:08,297]\u001b[0m Trial 12 finished with value: 0.8596 and parameters: {'learning_rate': 0.0001, 'per_device_train_batch_size': 128}. Best is trial 9 with value: 0.8892.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.3577311635017395, 'eval_accuracy': 0.8452, 'eval_runtime': 7.6084, 'eval_samples_per_second': 657.164, 'eval_steps_per_second': 41.138, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.4159, 'grad_norm': 12.753427505493164, 'learning_rate': 6.006389776357828e-05, 'epoch': 1.5974440894568689}\n{'eval_loss': 0.30707210302352905, 'eval_accuracy': 0.8738, 'eval_runtime': 7.6157, 'eval_samples_per_second': 656.539, 'eval_steps_per_second': 41.099, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.3272170424461365, 'eval_accuracy': 0.866, 'eval_runtime': 7.5969, 'eval_samples_per_second': 658.167, 'eval_steps_per_second': 41.201, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.2578, 'grad_norm': 2.919377565383911, 'learning_rate': 2.0127795527156552e-05, 'epoch': 3.194888178913738}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.3120056986808777, 'eval_accuracy': 0.8804, 'eval_runtime': 9.4494, 'eval_samples_per_second': 529.136, 'eval_steps_per_second': 33.124, 'epoch': 4.0}\n{'train_runtime': 153.0664, 'train_samples_per_second': 522.649, 'train_steps_per_second': 8.179, 'train_loss': 0.31251668473021293, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 02:59:42,073]\u001b[0m Trial 13 finished with value: 0.8804 and parameters: {'learning_rate': 0.0001, 'per_device_train_batch_size': 32}. Best is trial 9 with value: 0.8892.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.5822, 'grad_norm': 5.140401363372803, 'learning_rate': 2.4e-05, 'epoch': 0.8}\n{'eval_loss': 0.4335492253303528, 'eval_accuracy': 0.8082, 'eval_runtime': 7.5811, 'eval_samples_per_second': 659.531, 'eval_steps_per_second': 41.287, 'epoch': 1.0}\n\u001b[32m[I 2025-03-17 03:00:24,110]\u001b[0m Trial 14 pruned. \u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.519, 'grad_norm': 6.78434419631958, 'learning_rate': 4e-05, 'epoch': 0.8}\n{'eval_loss': 0.3768501579761505, 'eval_accuracy': 0.8368, 'eval_runtime': 7.5255, 'eval_samples_per_second': 664.41, 'eval_steps_per_second': 41.592, 'epoch': 1.0}\n\u001b[32m[I 2025-03-17 03:01:06,046]\u001b[0m Trial 15 pruned. \u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.3126256465911865, 'eval_accuracy': 0.8708, 'eval_runtime': 7.5311, 'eval_samples_per_second': 663.911, 'eval_steps_per_second': 41.561, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.29564663767814636, 'eval_accuracy': 0.8866, 'eval_runtime': 7.6753, 'eval_samples_per_second': 651.442, 'eval_steps_per_second': 40.78, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.297776460647583, 'eval_accuracy': 0.8894, 'eval_runtime': 7.5629, 'eval_samples_per_second': 661.125, 'eval_steps_per_second': 41.386, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.2842, 'grad_norm': 2.0189857482910156, 'learning_rate': 6.114649681528662e-05, 'epoch': 3.1847133757961785}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.31761541962623596, 'eval_accuracy': 0.888, 'eval_runtime': 7.5723, 'eval_samples_per_second': 660.297, 'eval_steps_per_second': 41.335, 'epoch': 4.0}\n{'train_runtime': 146.8482, 'train_samples_per_second': 544.78, 'train_steps_per_second': 4.277, 'train_loss': 0.2520992072524538, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 03:03:33,532]\u001b[0m Trial 16 finished with value: 0.888 and parameters: {'learning_rate': 0.0003, 'per_device_train_batch_size': 64}. Best is trial 9 with value: 0.8892.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.4675, 'grad_norm': 1.1084424257278442, 'learning_rate': 0.00027, 'epoch': 0.4}\n{'loss': 0.3493, 'grad_norm': 15.720585823059082, 'learning_rate': 0.00023999999999999998, 'epoch': 0.8}\n{'eval_loss': 0.28487733006477356, 'eval_accuracy': 0.8824, 'eval_runtime': 7.7236, 'eval_samples_per_second': 647.368, 'eval_steps_per_second': 40.525, 'epoch': 1.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.2814, 'grad_norm': 4.13091516494751, 'learning_rate': 0.00020999999999999998, 'epoch': 1.2}\n{'loss': 0.2197, 'grad_norm': 6.729955673217773, 'learning_rate': 0.00017999999999999998, 'epoch': 1.6}\n{'loss': 0.2076, 'grad_norm': 8.358450889587402, 'learning_rate': 0.00015, 'epoch': 2.0}\n{'eval_loss': 0.3505120873451233, 'eval_accuracy': 0.8876, 'eval_runtime': 7.5647, 'eval_samples_per_second': 660.964, 'eval_steps_per_second': 41.376, 'epoch': 2.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.133, 'grad_norm': 0.13731126487255096, 'learning_rate': 0.00011999999999999999, 'epoch': 2.4}\n{'loss': 0.116, 'grad_norm': 0.1799858808517456, 'learning_rate': 8.999999999999999e-05, 'epoch': 2.8}\n{'eval_loss': 0.40874725580215454, 'eval_accuracy': 0.8876, 'eval_runtime': 7.6677, 'eval_samples_per_second': 652.088, 'eval_steps_per_second': 40.821, 'epoch': 3.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.1033, 'grad_norm': 32.59148406982422, 'learning_rate': 5.9999999999999995e-05, 'epoch': 3.2}\n{'loss': 0.075, 'grad_norm': 28.054603576660156, 'learning_rate': 2.9999999999999997e-05, 'epoch': 3.6}\n{'loss': 0.0692, 'grad_norm': 0.6811943054199219, 'learning_rate': 0.0, 'epoch': 4.0}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.5254442095756531, 'eval_accuracy': 0.8854, 'eval_runtime': 7.7677, 'eval_samples_per_second': 643.695, 'eval_steps_per_second': 40.295, 'epoch': 4.0}\n{'train_runtime': 210.0481, 'train_samples_per_second': 380.865, 'train_steps_per_second': 23.804, 'train_loss': 0.2022069351196289, 'epoch': 4.0}\n\u001b[32m[I 2025-03-17 03:07:04,366]\u001b[0m Trial 17 finished with value: 0.8854 and parameters: {'learning_rate': 0.0003, 'per_device_train_batch_size': 8}. Best is trial 9 with value: 0.8892.\u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.6119, 'grad_norm': 17.425430297851562, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.4}\n{'loss': 0.4492, 'grad_norm': 13.745095252990723, 'learning_rate': 2.4e-05, 'epoch': 0.8}\n{'eval_loss': 0.4058513045310974, 'eval_accuracy': 0.8242, 'eval_runtime': 7.5686, 'eval_samples_per_second': 660.627, 'eval_steps_per_second': 41.355, 'epoch': 1.0}\n\u001b[32m[I 2025-03-17 03:07:57,451]\u001b[0m Trial 18 pruned. \u001b[0m\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'eval_loss': 0.4076404273509979, 'eval_accuracy': 0.8252, 'eval_runtime': 7.7915, 'eval_samples_per_second': 641.725, 'eval_steps_per_second': 40.172, 'epoch': 1.0}\n\u001b[32m[I 2025-03-17 03:08:35,885]\u001b[0m Trial 19 pruned. \u001b[0m\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os \nos.environ[\"WANDB_DISABLED\"] = \"true\"\nfrom tqdm.notebook import tqdm\ntqdm._instances.clear()\n!python /kaggle/working/DS-GA-1012-NLU/hw2/test_model_bitfit.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T03:08:37.897782Z","iopub.execute_input":"2025-03-17T03:08:37.898044Z","iopub.status.idle":"2025-03-17T03:09:43.110645Z","shell.execute_reply.started":"2025-03-17T03:08:37.898022Z","shell.execute_reply":"2025-03-17T03:09:43.109794Z"}},"outputs":[{"name":"stdout","text":"2025-03-17 03:08:41.988250: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-17 03:08:42.010656: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-17 03:08:42.017503: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nMap: 100%|███████████████████████| 25000/25000 [00:15<00:00, 1606.22 examples/s]\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os \nos.environ[\"WANDB_DISABLED\"] = \"true\"\nfrom tqdm.notebook import tqdm\ntqdm._instances.clear()\n!python /kaggle/working/DS-GA-1012-NLU/hw2/test_model_nobitfit.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T03:09:43.111619Z","iopub.execute_input":"2025-03-17T03:09:43.111865Z","iopub.status.idle":"2025-03-17T03:10:33.660885Z","shell.execute_reply.started":"2025-03-17T03:09:43.111844Z","shell.execute_reply":"2025-03-17T03:10:33.660062Z"}},"outputs":[{"name":"stdout","text":"2025-03-17 03:09:47.099634: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-17 03:09:47.121085: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-17 03:09:47.127612: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport glob\nimport zipfile\nimport shutil\nfrom pathlib import Path\n\ndef zip_checkpoints_and_pickles():\n    \"\"\"\n    Function to zip the specific checkpoints folders ('checkpoints_bitfit' and 'checkpoints_nobitfit')\n    and all pickle files (with various extensions) in the current directory into a single zip file for downloading.\n    \n    Returns:\n        str: Path to the created zip file\n    \"\"\"\n    # Define the output zip filename\n    output_zip = \"checkpoints_and_pickles.zip\"\n    \n    # Specific checkpoint folders to include\n    specific_checkpoint_folders = ['checkpoints_bitfit', 'checkpoints_nobitfit']\n    existing_checkpoint_folders = [folder for folder in specific_checkpoint_folders if os.path.exists(folder)]\n    \n    # Find all pickle files with various extensions\n    pickle_extensions = ['.pkl', '.pickle', '.p', '.pckl', '.pcl', '.py_pickle']\n    pickle_files = []\n    for ext in pickle_extensions:\n        pickle_files.extend(glob.glob(f\"*{ext}\"))\n    \n    # Print what we're about to zip\n    print(f\"Found {len(existing_checkpoint_folders)} checkpoint folders and {len(pickle_files)} pickle files.\")\n    if len(existing_checkpoint_folders) < len(specific_checkpoint_folders):\n        missing_folders = set(specific_checkpoint_folders) - set(existing_checkpoint_folders)\n        print(f\"Note: Couldn't find the following folders: {', '.join(missing_folders)}\")\n    \n    # Create the zip file\n    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Add specific checkpoint folders\n        for folder in existing_checkpoint_folders:\n            print(f\"Adding folder: {folder}\")\n            for root, _, files in os.walk(folder):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    # Calculate path inside the zip file\n                    arcname = os.path.relpath(file_path)\n                    zipf.write(file_path, arcname)\n        \n        # Add pickle files\n        for pkl_file in pickle_files:\n            print(f\"Adding pickle file: {pkl_file}\")\n            zipf.write(pkl_file)\n    \n    print(f\"Successfully created {output_zip}\")\n    return output_zip\n\n# For Kaggle notebooks, you can use the following to create a download link\ntry:\n    from kaggle_web_client import KaggleWebClient\n    \n    # Create the zip file\n    zip_path = zip_checkpoints_and_pickles()\n    \n    # Create a download link in Kaggle notebook\n    from IPython.display import FileLink\n    display(FileLink(zip_path))\n    print(f\"Click the link above to download {zip_path}\")\n    \nexcept ImportError:\n    # Not running in Kaggle environment\n    print(\"Not running in a Kaggle notebook. Creating zip file only.\")\n    zip_path = zip_checkpoints_and_pickles()\n    print(f\"Zip file created at: {zip_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T03:19:21.658633Z","iopub.execute_input":"2025-03-17T03:19:21.658905Z","iopub.status.idle":"2025-03-17T03:21:49.743628Z","shell.execute_reply.started":"2025-03-17T03:19:21.658884Z","shell.execute_reply":"2025-03-17T03:21:49.742805Z"}},"outputs":[{"name":"stdout","text":"Found 2 checkpoint folders and 4 pickle files.\nAdding folder: checkpoints_bitfit\nAdding folder: checkpoints_nobitfit\nAdding pickle file: train_results_with_bitfit.p\nAdding pickle file: train_results_without_bitfit.p\nAdding pickle file: test_results_with_bitfit.p\nAdding pickle file: test_results_without_bitfit.p\nSuccessfully created checkpoints_and_pickles.zip\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/checkpoints_and_pickles.zip","text/html":"<a href='checkpoints_and_pickles.zip' target='_blank'>checkpoints_and_pickles.zip</a><br>"},"metadata":{}},{"name":"stdout","text":"Click the link above to download checkpoints_and_pickles.zip\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}